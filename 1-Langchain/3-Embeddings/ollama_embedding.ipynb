{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001E04AA2B0D0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001E04AA29F00>, model='text-embedding-3-large', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='This is tutorial openai embedding'\n",
    "query_result=embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0008271772530861199,\n",
       " 0.028278157114982605,\n",
       " -0.009830222465097904,\n",
       " -0.03148411214351654,\n",
       " 0.028524769470095634,\n",
       " -0.007144893053919077,\n",
       " 0.022030657157301903,\n",
       " 0.055378060787916183,\n",
       " -0.013515698723495007,\n",
       " -0.009919276461005211,\n",
       " -0.01929052732884884,\n",
       " 0.003365224227309227,\n",
       " -0.014892615377902985,\n",
       " -0.006326278671622276,\n",
       " 0.003281307639554143,\n",
       " 0.021290821954607964,\n",
       " 0.017495740205049515,\n",
       " 0.04995260015130043,\n",
       " -0.02904539555311203,\n",
       " -0.030881283804774284,\n",
       " 0.02814115211367607,\n",
       " 0.0037368545308709145,\n",
       " -0.040060725063085556,\n",
       " -0.030141446739435196,\n",
       " 0.030251052230596542,\n",
       " -0.02470228634774685,\n",
       " 0.018865805119276047,\n",
       " 0.043896909803152084,\n",
       " -0.015002220869064331,\n",
       " 0.044554539024829865,\n",
       " 0.012214136309921741,\n",
       " -0.023496627807617188,\n",
       " 0.004353384021669626,\n",
       " -0.007336702197790146,\n",
       " -0.01668740063905716,\n",
       " 0.02590794488787651,\n",
       " 0.0409923680126667,\n",
       " 0.05115825682878494,\n",
       " -0.041869211941957474,\n",
       " 0.015440641902387142,\n",
       " 0.02863437496125698,\n",
       " -0.009487705305218697,\n",
       " -0.0069017065688967705,\n",
       " -0.013084128499031067,\n",
       " -0.04614381492137909,\n",
       " -0.018276678398251534,\n",
       " 0.01759164407849312,\n",
       " -0.01194012351334095,\n",
       " -0.01570095494389534,\n",
       " -0.017865657806396484,\n",
       " 0.028771381825208664,\n",
       " -0.01818077266216278,\n",
       " 0.0074531580321490765,\n",
       " -0.005041842348873615,\n",
       " -0.011070131324231625,\n",
       " 0.02904539555311203,\n",
       " 0.013269087299704552,\n",
       " 0.004613696597516537,\n",
       " 0.039923716336488724,\n",
       " 0.031182697042822838,\n",
       " -0.0007209971081465483,\n",
       " 0.00283432356081903,\n",
       " -0.010528955608606339,\n",
       " -0.017112120985984802,\n",
       " -0.01764644682407379,\n",
       " -0.018071167171001434,\n",
       " 0.022619785740971565,\n",
       " 0.005034992005676031,\n",
       " -0.011385247111320496,\n",
       " 0.03890986740589142,\n",
       " 0.0023119859397411346,\n",
       " -0.0057405754923820496,\n",
       " -0.017906760796904564,\n",
       " -0.01672850362956524,\n",
       " 0.01656409539282322,\n",
       " 0.0170162171125412,\n",
       " 0.03107309155166149,\n",
       " 0.008699918165802956,\n",
       " -0.007521660998463631,\n",
       " 0.0173176322132349,\n",
       " 0.03427904471755028,\n",
       " -0.01224838849157095,\n",
       " 0.015331036411225796,\n",
       " 0.016865510493516922,\n",
       " 0.07085980474948883,\n",
       " 0.021345624700188637,\n",
       " -0.02033177576959133,\n",
       " -0.04411611706018448,\n",
       " 0.000846443756017834,\n",
       " 0.016550395637750626,\n",
       " -0.06044730171561241,\n",
       " 0.015180328860878944,\n",
       " -0.025264013558626175,\n",
       " -0.003473116783425212,\n",
       " -0.005086369346827269,\n",
       " -0.003200816223397851,\n",
       " -0.0003568593238014728,\n",
       " 0.005894707981497049,\n",
       " -0.004274605307728052,\n",
       " 0.020276973024010658,\n",
       " 0.002620250917971134,\n",
       " -0.01809856854379177,\n",
       " -0.012803264893591404,\n",
       " 0.005812504328787327,\n",
       " -0.022208767011761665,\n",
       " 0.0020294098649173975,\n",
       " 0.03589572384953499,\n",
       " 0.00711749168112874,\n",
       " 0.007309300824999809,\n",
       " 0.01202232763171196,\n",
       " 0.009528807364404202,\n",
       " -0.005911834072321653,\n",
       " -0.027291711419820786,\n",
       " 0.04970598593354225,\n",
       " -0.0017322769854217768,\n",
       " -0.012433347292244434,\n",
       " 0.0027349938172847033,\n",
       " -0.015358437784016132,\n",
       " -0.0029524918645620346,\n",
       " -0.02863437496125698,\n",
       " 0.0097891204059124,\n",
       " -0.010467302985489368,\n",
       " 0.007576463744044304,\n",
       " -0.004942512139678001,\n",
       " 0.007898429408669472,\n",
       " 0.0508294403553009,\n",
       " -0.007473709061741829,\n",
       " 0.01082351990044117,\n",
       " 0.012056578882038593,\n",
       " -0.00888487696647644,\n",
       " 0.027538321912288666,\n",
       " -0.004891134798526764,\n",
       " -0.006911981850862503,\n",
       " 0.05135006457567215,\n",
       " 0.04217062518000603,\n",
       " 0.01431718748062849,\n",
       " -0.01970154605805874,\n",
       " 0.049596380442380905,\n",
       " 0.01668740063905716,\n",
       " 0.03564911335706711,\n",
       " -0.04036213830113411,\n",
       " -0.013070427812635899,\n",
       " -0.022455377504229546,\n",
       " 0.0011636996641755104,\n",
       " -0.019728947430849075,\n",
       " -0.009535658173263073,\n",
       " -0.017112120985984802,\n",
       " 0.01528993435204029,\n",
       " -0.01656409539282322,\n",
       " 0.0032128042075783014,\n",
       " -0.039512697607278824,\n",
       " 0.013440345413982868,\n",
       " 0.016632597893476486,\n",
       " 0.030799079686403275,\n",
       " 0.0018273252062499523,\n",
       " -0.015180328860878944,\n",
       " 0.00047310083755292,\n",
       " -0.013789712451398373,\n",
       " 0.02057838812470436,\n",
       " 0.005243926774710417,\n",
       " 0.049267567694187164,\n",
       " -0.017468338832259178,\n",
       " 0.02226356975734234,\n",
       " 0.008288898505270481,\n",
       " 0.002245195209980011,\n",
       " -0.004062245134264231,\n",
       " 0.011172886937856674,\n",
       " -0.005692623555660248,\n",
       " -0.06674960255622864,\n",
       " 0.034936677664518356,\n",
       " -0.0007055838941596448,\n",
       " 0.016427088528871536,\n",
       " 0.06384506821632385,\n",
       " 0.02759312465786934,\n",
       " -0.06499592214822769,\n",
       " -0.005346681922674179,\n",
       " 0.02557912841439247,\n",
       " -0.004476690199226141,\n",
       " -0.015851661562919617,\n",
       " 0.01685180887579918,\n",
       " -0.023250017315149307,\n",
       " 0.022825296968221664,\n",
       " 0.02738761529326439,\n",
       " 0.016344884410500526,\n",
       " -0.005815929267555475,\n",
       " 0.007295600138604641,\n",
       " 0.04101977124810219,\n",
       " 0.015810558572411537,\n",
       " 0.022153964266180992,\n",
       " -0.03375842049717903,\n",
       " 0.017797155305743217,\n",
       " -0.010768717154860497,\n",
       " 0.003349810838699341,\n",
       " -0.0034851047676056623,\n",
       " 0.03573131561279297,\n",
       " -0.04041694104671478,\n",
       " -0.01456379983574152,\n",
       " 0.027278009802103043,\n",
       " -0.009802821092307568,\n",
       " 0.009734317660331726,\n",
       " -0.004589720629155636,\n",
       " -0.05461082234978676,\n",
       " -0.0011928135063499212,\n",
       " -0.01485151331871748,\n",
       " 0.004274605307728052,\n",
       " 0.03781381621956825,\n",
       " -0.017619045451283455,\n",
       " -0.013803413137793541,\n",
       " 0.009665814228355885,\n",
       " -0.031210098415613174,\n",
       " 0.025853142142295837,\n",
       " -0.02962082251906395,\n",
       " -0.02896319143474102,\n",
       " -0.03945789486169815,\n",
       " 0.035840921103954315,\n",
       " 0.021003108471632004,\n",
       " -0.043732501566410065,\n",
       " 0.037156183272600174,\n",
       " -0.018852105364203453,\n",
       " 0.005271328147500753,\n",
       " -0.023976150900125504,\n",
       " 0.03707398101687431,\n",
       " 0.0418144091963768,\n",
       " -0.04175960645079613,\n",
       " -0.04126638174057007,\n",
       " -0.011864770203828812,\n",
       " -0.022318372502923012,\n",
       " -0.01892060786485672,\n",
       " -0.016468191519379616,\n",
       " 0.01350884884595871,\n",
       " 0.0195782408118248,\n",
       " 0.018536990508437157,\n",
       " 0.05151447281241417,\n",
       " -0.0033720743376761675,\n",
       " 0.006401632446795702,\n",
       " -0.015961267054080963,\n",
       " 0.03701917827129364,\n",
       " -0.025483224540948868,\n",
       " -0.001188532100059092,\n",
       " 0.048445526510477066,\n",
       " -0.020633190870285034,\n",
       " 0.025565428659319878,\n",
       " -0.01119343750178814,\n",
       " -0.009864473715424538,\n",
       " -0.03370361775159836,\n",
       " 0.05019921064376831,\n",
       " -0.017495740205049515,\n",
       " -0.004654798656702042,\n",
       " -0.008192993700504303,\n",
       " -0.007048988714814186,\n",
       " 0.017276529222726822,\n",
       " 0.018906908109784126,\n",
       " 0.025168107822537422,\n",
       " 0.008788972161710262,\n",
       " -0.044225722551345825,\n",
       " 0.035265494138002396,\n",
       " 0.059351250529289246,\n",
       " -0.014358289539813995,\n",
       " -0.0005848468281328678,\n",
       " -0.020427681505680084,\n",
       " 0.016303783282637596,\n",
       " 0.035429902374744415,\n",
       " 0.021304523572325706,\n",
       " -0.007323001511394978,\n",
       " -0.014673404395580292,\n",
       " -0.039183881133794785,\n",
       " 0.02045508287847042,\n",
       " -0.02937421016395092,\n",
       " 0.0069496589712798595,\n",
       " -0.0008036292274482548,\n",
       " 0.008597162552177906,\n",
       " 0.01871509850025177,\n",
       " 0.026428569108247757,\n",
       " 0.028826184570789337,\n",
       " 0.029675625264644623,\n",
       " -0.013241685926914215,\n",
       " 0.0009188003605231643,\n",
       " -0.010268643498420715,\n",
       " 0.022496480494737625,\n",
       " -0.038526251912117004,\n",
       " 0.018482187762856483,\n",
       " -0.01846848614513874,\n",
       " 0.029593421146273613,\n",
       " 0.015111825428903103,\n",
       " -0.020386578515172005,\n",
       " -0.027579424902796745,\n",
       " -0.043403685092926025,\n",
       " 0.036251939833164215,\n",
       " -0.05441901460289955,\n",
       " -0.03318299353122711,\n",
       " -0.011885320767760277,\n",
       " -0.0004773823020514101,\n",
       " 0.029511217027902603,\n",
       " 0.049760788679122925,\n",
       " -0.009891875088214874,\n",
       " 0.03425164520740509,\n",
       " -0.018564391881227493,\n",
       " -0.022441677749156952,\n",
       " -0.011618157848715782,\n",
       " -0.03285417705774307,\n",
       " -0.015851661562919617,\n",
       " 0.024962598457932472,\n",
       " -0.022277269512414932,\n",
       " -0.01730393059551716,\n",
       " 0.013433495536446571,\n",
       " 0.03962230309844017,\n",
       " 0.01146745029836893,\n",
       " 0.02734651416540146,\n",
       " -0.027908239513635635,\n",
       " 0.038197435438632965,\n",
       " 0.007576463744044304,\n",
       " 1.5814626749488525e-05,\n",
       " -0.017961561679840088,\n",
       " -0.04671924188733101,\n",
       " -0.001981457695364952,\n",
       " -0.004164999816566706,\n",
       " 0.02871657907962799,\n",
       " 0.0026288137305527925,\n",
       " 0.023359620943665504,\n",
       " -0.02189365215599537,\n",
       " 0.03230615332722664,\n",
       " -0.017208026722073555,\n",
       " -0.03888246789574623,\n",
       " 0.01697511598467827,\n",
       " -0.0027024548035115004,\n",
       " -0.014700805768370628,\n",
       " 0.03252536058425903,\n",
       " 0.005994037725031376,\n",
       " -0.001190244685858488,\n",
       " -0.028278157114982605,\n",
       " -0.005480263382196426,\n",
       " -0.05721394717693329,\n",
       " -0.022236168384552002,\n",
       " -0.013358141295611858,\n",
       " -0.008816373534500599,\n",
       " -0.032169144600629807,\n",
       " -0.005028141662478447,\n",
       " -0.002568873343989253,\n",
       " -0.006987335626035929,\n",
       " -0.017673848196864128,\n",
       " -0.01020698994398117,\n",
       " -0.0399785190820694,\n",
       " 0.00011228117364225909,\n",
       " -0.0307442769408226,\n",
       " -0.0008755576563999057,\n",
       " -0.01805746741592884,\n",
       " 0.035512104630470276,\n",
       " 0.029401611536741257,\n",
       " -0.012125082314014435,\n",
       " 0.006377656478434801,\n",
       " 0.00079292559530586,\n",
       " 0.012522401288151741,\n",
       " 0.02478449046611786,\n",
       " -0.025346217676997185,\n",
       " -0.017071019858121872,\n",
       " -0.04441753402352333,\n",
       " -0.033237796276807785,\n",
       " 0.013406094163656235,\n",
       " -0.02078389748930931,\n",
       " 0.031182697042822838,\n",
       " -0.04288306087255478,\n",
       " -0.01120028831064701,\n",
       " 0.0034422902390360832,\n",
       " -0.016180476173758507,\n",
       " -0.01530363503843546,\n",
       " -0.025127006694674492,\n",
       " 0.0016517855692654848,\n",
       " -0.017536841332912445,\n",
       " 0.027373915538191795,\n",
       " 0.024441974237561226,\n",
       " 0.006822927854955196,\n",
       " 0.010022031143307686,\n",
       " -0.03907427564263344,\n",
       " -0.0024318667128682137,\n",
       " -0.015413240529596806,\n",
       " -0.04134858399629593,\n",
       " -0.005034992005676031,\n",
       " -0.025510625913739204,\n",
       " 0.0076929195784032345,\n",
       " 0.020304374396800995,\n",
       " 0.0024250163696706295,\n",
       " -0.017660148441791534,\n",
       " 0.006076241843402386,\n",
       " 0.005983762443065643,\n",
       " 0.022989703342318535,\n",
       " -0.0011893883347511292,\n",
       " 0.0011285917134955525,\n",
       " -0.004778104368597269,\n",
       " -0.010405649431049824,\n",
       " 0.010508405044674873,\n",
       " 0.023907648399472237,\n",
       " -0.05913203954696655,\n",
       " 0.002140727825462818,\n",
       " -0.031045690178871155,\n",
       " 0.03532029688358307,\n",
       " 0.002140727825462818,\n",
       " -0.018687697127461433,\n",
       " -0.0143993915989995,\n",
       " 0.012625155970454216,\n",
       " 0.031456708908081055,\n",
       " -0.05115825682878494,\n",
       " -0.011015328578650951,\n",
       " -0.005285028833895922,\n",
       " 0.0031151871662586927,\n",
       " -0.008823223412036896,\n",
       " -0.01664629951119423,\n",
       " -0.04165000095963478,\n",
       " -0.031374506652355194,\n",
       " 0.015933865681290627,\n",
       " 0.029922237619757652,\n",
       " -0.01781085506081581,\n",
       " -0.007460008375346661,\n",
       " -0.0304428618401289,\n",
       " 0.020386578515172005,\n",
       " 0.0008284616633318365,\n",
       " -0.006435884162783623,\n",
       " -0.02000296115875244,\n",
       " -0.0022263568826019764,\n",
       " -0.01113178487867117,\n",
       " -0.009597310796380043,\n",
       " -0.0014882339164614677,\n",
       " -0.020304374396800995,\n",
       " 0.028442565351724625,\n",
       " -0.01805746741592884,\n",
       " -0.008179293014109135,\n",
       " 0.004791805054992437,\n",
       " -0.004404761362820864,\n",
       " 0.03973190858960152,\n",
       " -0.041211578994989395,\n",
       " -0.05567947402596474,\n",
       " -0.0071243420243263245,\n",
       " -0.0085081085562706,\n",
       " 0.03488187491893768,\n",
       " 0.01705731824040413,\n",
       " -0.052363913506269455,\n",
       " -0.016879210248589516,\n",
       " -0.027127303183078766,\n",
       " 0.014371990226209164,\n",
       " -0.029346808791160583,\n",
       " 0.01619417779147625,\n",
       " -0.0252092108130455,\n",
       " -0.023085609078407288,\n",
       " -0.006870879791676998,\n",
       " -0.013995221816003323,\n",
       " -0.05776197463274002,\n",
       " 0.00805598683655262,\n",
       " -0.0008430185844190419,\n",
       " -0.039266087114810944,\n",
       " 0.05315855145454407,\n",
       " -0.044718947261571884,\n",
       " -0.02549692429602146,\n",
       " 0.01334444060921669,\n",
       " -0.08867065608501434,\n",
       " -0.04688365012407303,\n",
       " 0.005963211413472891,\n",
       " -0.0023393873125314713,\n",
       " 0.009083536453545094,\n",
       " 0.024030953645706177,\n",
       " -0.014344588853418827,\n",
       " -0.039923716336488724,\n",
       " -0.0007081527728587389,\n",
       " 0.002570586046203971,\n",
       " 0.03740279749035835,\n",
       " 0.004459564108401537,\n",
       " 0.029182400554418564,\n",
       " -0.02627786248922348,\n",
       " -0.007446307688951492,\n",
       " -0.041869211941957474,\n",
       " 0.0027452693320810795,\n",
       " 0.024441974237561226,\n",
       " -0.030223650857806206,\n",
       " -0.017290230840444565,\n",
       " 0.0653795376420021,\n",
       " 0.032744571566581726,\n",
       " 0.01133729424327612,\n",
       " -0.006816077511757612,\n",
       " -0.018578091636300087,\n",
       " -0.011803116649389267,\n",
       " 0.017824556678533554,\n",
       " 0.012440197169780731,\n",
       " 0.016920313239097595,\n",
       " 0.01133729424327612,\n",
       " 0.02301710471510887,\n",
       " 0.007487409748136997,\n",
       " -0.010138486512005329,\n",
       " 0.02751092053949833,\n",
       " 0.03507368266582489,\n",
       " -0.0003979612956754863,\n",
       " -0.006394782103598118,\n",
       " 0.03107309155166149,\n",
       " 0.003753980388864875,\n",
       " 0.03644375130534172,\n",
       " -0.005291879177093506,\n",
       " -0.0399785190820694,\n",
       " -0.0052268011495471,\n",
       " -0.04751388356089592,\n",
       " -0.010337145999073982,\n",
       " -0.007672368548810482,\n",
       " 0.03597792610526085,\n",
       " 0.05085684359073639,\n",
       " -0.0315663143992424,\n",
       " 0.01387876644730568,\n",
       " -0.009282195940613747,\n",
       " 0.009076685644686222,\n",
       " 0.005250777117908001,\n",
       " 0.042115822434425354,\n",
       " -0.017276529222726822,\n",
       " 0.0004009583208244294,\n",
       " -0.0011320168850943446,\n",
       " 0.01177571527659893,\n",
       " 0.007980633527040482,\n",
       " -0.01850958913564682,\n",
       " 0.009837072342634201,\n",
       " -0.0033378228545188904,\n",
       " -0.010193289257586002,\n",
       " 0.027401315048336983,\n",
       " -0.0008040573447942734,\n",
       " 0.021592237055301666,\n",
       " 0.028990592807531357,\n",
       " 0.0074531580321490765,\n",
       " -0.01530363503843546,\n",
       " 0.013269087299704552,\n",
       " 0.008172442205250263,\n",
       " 0.042526841163635254,\n",
       " -0.022838996723294258,\n",
       " 0.020674291998147964,\n",
       " 0.0008032010518945754,\n",
       " -0.017742352560162544,\n",
       " -0.0258257407695055,\n",
       " -0.011625008657574654,\n",
       " -0.008686217479407787,\n",
       " 0.013666406273841858,\n",
       " -0.02697659470140934,\n",
       " -0.022249868139624596,\n",
       " -0.007206546142697334,\n",
       " 0.023455526679754257,\n",
       " 0.02016736939549446,\n",
       " 0.015385839156806469,\n",
       " 0.004805505741387606,\n",
       " -0.02433236874639988,\n",
       " -0.00418212590739131,\n",
       " 0.035429902374744415,\n",
       " 0.0058296299539506435,\n",
       " -0.002137302653864026,\n",
       " -0.019180921837687492,\n",
       " 0.0046959007158875465,\n",
       " 0.016303783282637596,\n",
       " -0.043896909803152084,\n",
       " -0.03896467015147209,\n",
       " 0.04217062518000603,\n",
       " -0.003949214704334736,\n",
       " -0.0027058799751102924,\n",
       " 0.005476837977766991,\n",
       " 0.007987483404576778,\n",
       " -0.002368501154705882,\n",
       " 0.016824407503008842,\n",
       " -0.0025842864997684956,\n",
       " 0.006689346395432949,\n",
       " -0.018989112228155136,\n",
       " 0.00842590443789959,\n",
       " 0.03184032812714577,\n",
       " 0.00782992597669363,\n",
       " -0.00685717910528183,\n",
       " -0.012159333564341068,\n",
       " 0.018372582271695137,\n",
       " -0.041375987231731415,\n",
       " 0.00182047497946769,\n",
       " 0.0011911009205505252,\n",
       " 0.018578091636300087,\n",
       " 0.012460748665034771,\n",
       " 0.00894652958959341,\n",
       " 0.0021869675256311893,\n",
       " -0.0027880838606506586,\n",
       " 0.030853882431983948,\n",
       " -0.013269087299704552,\n",
       " 0.035758718848228455,\n",
       " 0.00500074028968811,\n",
       " 0.002848024247214198,\n",
       " -0.018605493009090424,\n",
       " 0.03712878376245499,\n",
       " -0.012419646605849266,\n",
       " 0.00765181751921773,\n",
       " -0.011741464026272297,\n",
       " 0.024017253890633583,\n",
       " 0.020605789497494698,\n",
       " 0.007768272887915373,\n",
       " 0.0016851809341460466,\n",
       " -0.022236168384552002,\n",
       " 0.0016252405475825071,\n",
       " 0.009131488390266895,\n",
       " 0.029566019773483276,\n",
       " 0.012953972443938255,\n",
       " 0.009076685644686222,\n",
       " -0.012433347292244434,\n",
       " -0.019139818847179413,\n",
       " 0.028278157114982605,\n",
       " 0.030415460467338562,\n",
       " 0.018331481143832207,\n",
       " 0.03743019700050354,\n",
       " 0.004045119043439627,\n",
       " -0.002185255056247115,\n",
       " -0.01476930920034647,\n",
       " 0.02453787811100483,\n",
       " 0.04027993604540825,\n",
       " -0.018660295754671097,\n",
       " -0.008357401005923748,\n",
       " 0.05198029801249504,\n",
       " 0.00465822359547019,\n",
       " 0.0006995898438617587,\n",
       " 0.008542359806597233,\n",
       " -0.020633190870285034,\n",
       " -0.023195214569568634,\n",
       " 0.024154260754585266,\n",
       " -0.0179341621696949,\n",
       " 0.01456379983574152,\n",
       " 0.014056875370442867,\n",
       " -0.036334145814180374,\n",
       " 0.04175960645079613,\n",
       " -0.005579593125730753,\n",
       " -0.010755016468465328,\n",
       " 0.02664778009057045,\n",
       " 0.025962747633457184,\n",
       " 0.04085536301136017,\n",
       " -0.02494889870285988,\n",
       " 0.013234836049377918,\n",
       " 0.024592680856585503,\n",
       " -0.007734021637588739,\n",
       " -0.03880026191473007,\n",
       " 0.005315855145454407,\n",
       " -0.013269087299704552,\n",
       " 0.03131970390677452,\n",
       " 0.002169841667637229,\n",
       " -0.01805746741592884,\n",
       " -0.0015567372320219874,\n",
       " -0.005836480297148228,\n",
       " 0.00020133545331191272,\n",
       " 0.01759164407849312,\n",
       " 0.023195214569568634,\n",
       " 0.03310079127550125,\n",
       " -0.010864621959626675,\n",
       " 0.015755755826830864,\n",
       " 0.008645115420222282,\n",
       " -0.020509885624051094,\n",
       " 0.035347696393728256,\n",
       " -0.012556653469800949,\n",
       " -0.010357697494328022,\n",
       " 0.01080296840518713,\n",
       " 0.002644226886332035,\n",
       " 0.0332103930413723,\n",
       " -0.019482335075736046,\n",
       " -0.005353532265871763,\n",
       " -0.006593441590666771,\n",
       " 0.0021167516242712736,\n",
       " 0.009426052682101727,\n",
       " -0.014974819496273994,\n",
       " -0.029429012909531593,\n",
       " 0.003716303501278162,\n",
       " 0.014495296403765678,\n",
       " -0.010385098867118359,\n",
       " 0.0327993743121624,\n",
       " -0.016303783282637596,\n",
       " 0.012933420948684216,\n",
       " 0.014180180616676807,\n",
       " -0.02351032942533493,\n",
       " 0.041293781250715256,\n",
       " -0.035594310611486435,\n",
       " -0.005041842348873615,\n",
       " -0.0461164154112339,\n",
       " 0.0035724465269595385,\n",
       " 0.02668888121843338,\n",
       " 0.007665518205612898,\n",
       " -0.020468782633543015,\n",
       " -0.052117303013801575,\n",
       " -0.025058504194021225,\n",
       " 0.01863289438188076,\n",
       " -0.006103643216192722,\n",
       " 0.0010583758121356368,\n",
       " 0.009179441258311272,\n",
       " 0.01990705542266369,\n",
       " -0.00865881610661745,\n",
       " -0.012659408152103424,\n",
       " -0.024880394339561462,\n",
       " -0.019591940566897392,\n",
       " -0.006840053480118513,\n",
       " -0.007028437685221434,\n",
       " -0.00492881191894412,\n",
       " -0.020646890625357628,\n",
       " 0.0068126521073281765,\n",
       " -0.016577797010540962,\n",
       " 0.020509885624051094,\n",
       " 0.0041444492526352406,\n",
       " -0.02156483568251133,\n",
       " 0.04225282743573189,\n",
       " 0.005507664289325476,\n",
       " 0.009960378520190716,\n",
       " 0.01713952235877514,\n",
       " -0.006929107941687107,\n",
       " 0.014303486794233322,\n",
       " 0.01652299426496029,\n",
       " -0.006555764935910702,\n",
       " -0.059186842292547226,\n",
       " 0.0163996871560812,\n",
       " -0.017756052315235138,\n",
       " 0.016618898138403893,\n",
       " 0.022044358775019646,\n",
       " -0.005463137291371822,\n",
       " -0.004346533678472042,\n",
       " -0.007377804256975651,\n",
       " -0.02408575639128685,\n",
       " 0.003832759102806449,\n",
       " -0.004689050372689962,\n",
       " -0.004154724534600973,\n",
       " 0.007788823917508125,\n",
       " 0.03405983746051788,\n",
       " -0.03244315832853317,\n",
       " 0.010590608231723309,\n",
       " -0.016016069799661636,\n",
       " 0.0163996871560812,\n",
       " 0.00999462977051735,\n",
       " -0.007357253227382898,\n",
       " -0.0030501089058816433,\n",
       " 0.029237203299999237,\n",
       " 0.01760534569621086,\n",
       " -0.028853585943579674,\n",
       " 0.01941383257508278,\n",
       " 0.0037060279864817858,\n",
       " -0.023619934916496277,\n",
       " -0.009878174401819706,\n",
       " 0.008439605124294758,\n",
       " -0.0006272332393564284,\n",
       " 0.021468931809067726,\n",
       " -0.034854475408792496,\n",
       " -0.014988520182669163,\n",
       " 0.016139375045895576,\n",
       " 0.009898724965751171,\n",
       " -0.04688365012407303,\n",
       " -0.039266087114810944,\n",
       " 0.008254646323621273,\n",
       " -0.005305579863488674,\n",
       " -0.026209358125925064,\n",
       " 0.02660667710006237,\n",
       " -0.028579572215676308,\n",
       " -0.03507368266582489,\n",
       " -0.0012330592144280672,\n",
       " -0.0019317928235977888,\n",
       " -0.004445863422006369,\n",
       " 0.005261052865535021,\n",
       " -0.002767532831057906,\n",
       " -0.012200435623526573,\n",
       " -0.002885701134800911,\n",
       " 0.02033177576959133,\n",
       " 0.00956305954605341,\n",
       " 0.014303486794233322,\n",
       " 0.0015490306541323662,\n",
       " -0.03186773136258125,\n",
       " 0.021222319453954697,\n",
       " 0.008823223412036896,\n",
       " 0.0009290758753195405,\n",
       " -0.015892762690782547,\n",
       " 0.025565428659319878,\n",
       " -0.005956361070275307,\n",
       " 0.042115822434425354,\n",
       " -0.022249868139624596,\n",
       " 0.04565059393644333,\n",
       " 0.023606233298778534,\n",
       " 0.01065911166369915,\n",
       " 0.02631896361708641,\n",
       " 0.005723449867218733,\n",
       " -0.015248832292854786,\n",
       " -0.016098273918032646,\n",
       " 0.013275937177240849,\n",
       " 0.027401315048336983,\n",
       " 0.007275049574673176,\n",
       " 0.01949603669345379,\n",
       " -0.026086052879691124,\n",
       " -0.007864177227020264,\n",
       " 0.00993297714740038,\n",
       " 0.014043174684047699,\n",
       " -0.018811004236340523,\n",
       " -0.02383914403617382,\n",
       " -0.00437051011249423,\n",
       " -0.013597902841866016,\n",
       " 0.021427828818559647,\n",
       " 0.013399243354797363,\n",
       " -0.016372285783290863,\n",
       " -0.029730428010225296,\n",
       " 0.018989112228155136,\n",
       " -0.005096644628793001,\n",
       " 0.021784046664834023,\n",
       " 0.009076685644686222,\n",
       " -0.027949342504143715,\n",
       " -0.011063281446695328,\n",
       " -0.0019009662792086601,\n",
       " 0.014248684048652649,\n",
       " 0.02346922643482685,\n",
       " -0.004161574877798557,\n",
       " 0.00865196529775858,\n",
       " 0.00444243848323822,\n",
       " 0.02693549357354641,\n",
       " -0.03962230309844017,\n",
       " 0.03789601847529411,\n",
       " 0.003962915390729904,\n",
       " 0.012467598542571068,\n",
       " 0.05526845529675484,\n",
       " -0.01789305917918682,\n",
       " 0.018989112228155136,\n",
       " 0.011494851671159267,\n",
       " 0.029346808791160583,\n",
       " 0.04850032925605774,\n",
       " 0.006922257598489523,\n",
       " 0.010652261786162853,\n",
       " -0.024852992966771126,\n",
       " -0.006627693306654692,\n",
       " 0.0332103930413723,\n",
       " 0.006316003389656544,\n",
       " 0.025003701448440552,\n",
       " -0.001701450441032648,\n",
       " 0.0030894982628524303,\n",
       " -0.013001924380660057,\n",
       " -0.027579424902796745,\n",
       " 0.005778252612799406,\n",
       " 0.009364400058984756,\n",
       " 0.005884432699531317,\n",
       " 0.009871324524283409,\n",
       " -0.013769160956144333,\n",
       " 0.01826297678053379,\n",
       " 0.014371990226209164,\n",
       " 0.0225512832403183,\n",
       " 0.0020311225671321154,\n",
       " -0.013337590731680393,\n",
       " 0.0244967769831419,\n",
       " -0.007672368548810482,\n",
       " -0.02322261594235897,\n",
       " 0.042828258126974106,\n",
       " 0.007946381345391273,\n",
       " 0.03611493483185768,\n",
       " 0.00481235608458519,\n",
       " 0.00025560290669091046,\n",
       " 0.031210098415613174,\n",
       " 0.015536545775830746,\n",
       " 0.005209675058722496,\n",
       " -0.0023787766695022583,\n",
       " -0.04351329058408737,\n",
       " -0.025729835033416748,\n",
       " 0.022784193977713585,\n",
       " 0.0035690213553607464,\n",
       " 0.001346089644357562,\n",
       " -0.007028437685221434,\n",
       " -0.0010001480113714933,\n",
       " -0.0071722944267094135,\n",
       " 0.024428272619843483,\n",
       " 0.008597162552177906,\n",
       " 0.0045280675403773785,\n",
       " -0.00664139399304986,\n",
       " -0.0020773622673004866,\n",
       " 0.006761274766176939,\n",
       " 0.02160593681037426,\n",
       " 0.02503110282123089,\n",
       " -0.0031734148506075144,\n",
       " 0.029429012909531593,\n",
       " 0.03170332312583923,\n",
       " 0.000992441433481872,\n",
       " -0.024195361882448196,\n",
       " 0.0019317928235977888,\n",
       " 0.003152863821014762,\n",
       " -0.03806042671203613,\n",
       " -0.019098717719316483,\n",
       " -0.013789712451398373,\n",
       " 0.010056283324956894,\n",
       " -0.031922534108161926,\n",
       " 0.003276169765740633,\n",
       " 0.007069539278745651,\n",
       " -0.009980929084122181,\n",
       " 0.028017845004796982,\n",
       " 0.017632747069001198,\n",
       " 0.028278157114982605,\n",
       " -0.009186291135847569,\n",
       " -0.01582426019012928,\n",
       " -0.05028141289949417,\n",
       " -0.036169737577438354,\n",
       " 0.009501405991613865,\n",
       " 0.028853585943579674,\n",
       " 0.0077545722015202045,\n",
       " 0.02863437496125698,\n",
       " 0.01764644682407379,\n",
       " 0.018934309482574463,\n",
       " 0.00444243848323822,\n",
       " 0.01615307666361332,\n",
       " 0.0015909889480099082,\n",
       " -0.02804524637758732,\n",
       " -0.005274753086268902,\n",
       " -0.013070427812635899,\n",
       " 0.03929348662495613,\n",
       " -0.010446751490235329,\n",
       " 0.022647187113761902,\n",
       " 0.03806042671203613,\n",
       " 0.008131340146064758,\n",
       " 0.0023787766695022583,\n",
       " 0.01336499210447073,\n",
       " -0.016372285783290863,\n",
       " -0.014906316064298153,\n",
       " -0.04143078997731209,\n",
       " 0.008706768043339252,\n",
       " -0.034553058445453644,\n",
       " 0.028661776334047318,\n",
       " -0.014618601649999619,\n",
       " 0.01268680952489376,\n",
       " 0.015892762690782547,\n",
       " 0.006535213906317949,\n",
       " 0.01505702268332243,\n",
       " -0.014947418123483658,\n",
       " -0.013755460269749165,\n",
       " 0.0379234217107296,\n",
       " 0.008090238086879253,\n",
       " 0.014262384735047817,\n",
       " -0.01713952235877514,\n",
       " 0.0043670847080647945,\n",
       " 0.0005899845855310559,\n",
       " 0.04077315703034401,\n",
       " 0.015344737097620964,\n",
       " -0.003032983047887683,\n",
       " 0.04441753402352333,\n",
       " 0.04200621694326401,\n",
       " 0.0020362602081149817,\n",
       " 0.0002911389747168869,\n",
       " -0.031127894297242165,\n",
       " -0.03474486991763115,\n",
       " -0.005076093599200249,\n",
       " 0.0022623210679739714,\n",
       " 0.005493964068591595,\n",
       " -0.04003332182765007,\n",
       " 0.020550986751914024,\n",
       " 0.022441677749156952,\n",
       " 0.032826777547597885,\n",
       " 0.01756424270570278,\n",
       " 0.0034936678130179644,\n",
       " -0.029264604672789574,\n",
       " 0.02668888121843338,\n",
       " -0.017920460551977158,\n",
       " -0.01096737664192915,\n",
       " 0.024715986102819443,\n",
       " 0.013556800782680511,\n",
       " -0.019674144685268402,\n",
       " -0.00933699868619442,\n",
       " -0.006545489188283682,\n",
       " -0.018071167171001434,\n",
       " 0.0036066982429474592,\n",
       " 0.013707508333027363,\n",
       " 0.00189240335021168,\n",
       " -0.01723542809486389,\n",
       " -0.016166776418685913,\n",
       " 0.032991185784339905,\n",
       " -0.008939679712057114,\n",
       " 0.01456379983574152,\n",
       " -0.01423498336225748,\n",
       " 0.004904835484921932,\n",
       " -0.027291711419820786,\n",
       " -0.0081998435780406,\n",
       " -0.03482707217335701,\n",
       " 0.006168721243739128,\n",
       " 0.015358437784016132,\n",
       " -0.009165740571916103,\n",
       " -0.007329851854592562,\n",
       " 0.007103790994733572,\n",
       " 0.0027161554899066687,\n",
       " -0.01785195805132389,\n",
       " 0.017208026722073555,\n",
       " 0.006768125109374523,\n",
       " -0.003911538049578667,\n",
       " -0.01059745904058218,\n",
       " -0.005874156951904297,\n",
       " -0.0036820517852902412,\n",
       " 0.009131488390266895,\n",
       " -0.021181216463446617,\n",
       " 0.03556690737605095,\n",
       " -0.004363659769296646,\n",
       " -0.0061721461825072765,\n",
       " 0.008494407869875431,\n",
       " -0.01631748303771019,\n",
       " 0.012440197169780731,\n",
       " 0.010727615095674992,\n",
       " 0.010323445312678814,\n",
       " 0.008727319538593292,\n",
       " 0.0258257407695055,\n",
       " -0.008158741518855095,\n",
       " 0.009446603246033192,\n",
       " 0.02664778009057045,\n",
       " -0.013173182494938374,\n",
       " -0.030497664585709572,\n",
       " -0.021551134064793587,\n",
       " -0.0010489566484466195,\n",
       " -0.01949603669345379,\n",
       " 0.01582426019012928,\n",
       " -0.02267458848655224,\n",
       " -0.010871471837162971,\n",
       " 0.010693363845348358,\n",
       " -0.006281751673668623,\n",
       " -0.015837959945201874,\n",
       " 0.004593145567923784,\n",
       " 0.0032196545507758856,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0008271772530861199"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('speech.txt')\n",
    "\n",
    "text=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Ladies and gentlemen, esteemed guests, and dear friends,\\n\\nToday, we gather to reflect on a powerful theme that resonates deeply within each of us: change. In a world that is constantly evolving, embracing change is not just a necessity; it is an opportunity for growth and transformation.\\n\\nChange can be daunting. It can evoke feelings of fear and uncertainty. However, it is also a catalyst for innovation and progress. Consider the moments in your life when change brought unexpected opportunities. Perhaps a new job, a relocation to a different city, or even a chance meeting that sparked a lifelong friendship. Each of these experiences was a stepping stone toward becoming who you are today.\\n\\nAs we navigate through the complexities of life, we must remember that change often comes hand in hand with resilience. It teaches us to adapt, to learn, and to thrive. The challenges we face are not meant to break us; they are meant to build us stronger. Let us not shy away from these challenges but rather face them with courage and determination.\\n\\nI urge each of you to embrace change, whether big or small. Let it inspire you to pursue your passions, to take risks, and to dream beyond the boundaries of the familiar. Surround yourselves with positivity, seek support from those who uplift you, and always keep moving forward.\\n\\nIn closing, let us celebrate the beauty of change. It is a reminder that life is a journey, filled with endless possibilities. Together, let us step boldly into the future, ready to embrace all that lies ahead.\\n\\nThank you.\\n\\n')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'speech.txt'}, page_content='Ladies and gentlemen, esteemed guests, and dear friends,\\n\\nToday, we gather to reflect on a powerful theme that resonates deeply within each of us: change. In a world that is constantly evolving, embracing change is not just a necessity; it is an opportunity for growth and transformation.\\n\\nChange can be daunting. It can evoke feelings of fear and uncertainty. However, it is also a catalyst for innovation and progress. Consider the moments in your life when change brought unexpected opportunities. Perhaps a new job, a relocation to a different city, or even a chance meeting that sparked a lifelong friendship. Each of these experiences was a stepping stone toward becoming who you are today.\\n\\nAs we navigate through the complexities of life, we must remember that change often comes hand in hand with resilience. It teaches us to adapt, to learn, and to thrive. The challenges we face are not meant to break us; they are meant to build us stronger. Let us not shy away from these challenges but rather face them with courage and determination.\\n\\nI urge each of you to embrace change, whether big or small. Let it inspire you to pursue your passions, to take risks, and to dream beyond the boundaries of the familiar. Surround yourselves with positivity, seek support from those who uplift you, and always keep moving forward.\\n\\nIn closing, let us celebrate the beauty of change. It is a reminder that life is a journey, filled with endless possibilities. Together, let us step boldly into the future, ready to embrace all that lies ahead.\\n\\nThank you.\\n\\n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "db=Chroma.from_documents(text,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x1e05cc25840>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"celebrate\"\n",
    "retrieved_results = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Ladies and gentlemen, esteemed guests, and dear friends,\\n\\nToday, we gather to reflect on a powerful theme that resonates deeply within each of us: change. In a world that is constantly evolving, embracing change is not just a necessity; it is an opportunity for growth and transformation.\\n\\nChange can be daunting. It can evoke feelings of fear and uncertainty. However, it is also a catalyst for innovation and progress. Consider the moments in your life when change brought unexpected opportunities. Perhaps a new job, a relocation to a different city, or even a chance meeting that sparked a lifelong friendship. Each of these experiences was a stepping stone toward becoming who you are today.\\n\\nAs we navigate through the complexities of life, we must remember that change often comes hand in hand with resilience. It teaches us to adapt, to learn, and to thrive. The challenges we face are not meant to break us; they are meant to build us stronger. Let us not shy away from these challenges but rather face them with courage and determination.\\n\\nI urge each of you to embrace change, whether big or small. Let it inspire you to pursue your passions, to take risks, and to dream beyond the boundaries of the familiar. Surround yourselves with positivity, seek support from those who uplift you, and always keep moving forward.\\n\\nIn closing, let us celebrate the beauty of change. It is a reminder that life is a journey, filled with endless possibilities. Together, let us step boldly into the future, ready to embrace all that lies ahead.\\n\\nThank you.\\n\\n'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Ladies and gentlemen, esteemed guests, and dear friends,\\n\\nToday, we gather to reflect on a powerful theme that resonates deeply within each of us: change. In a world that is constantly evolving, embracing change is not just a necessity; it is an opportunity for growth and transformation.\\n\\nChange can be daunting. It can evoke feelings of fear and uncertainty. However, it is also a catalyst for innovation and progress. Consider the moments in your life when change brought unexpected opportunities. Perhaps a new job, a relocation to a different city, or even a chance meeting that sparked a lifelong friendship. Each of these experiences was a stepping stone toward becoming who you are today.\\n\\nAs we navigate through the complexities of life, we must remember that change often comes hand in hand with resilience. It teaches us to adapt, to learn, and to thrive. The challenges we face are not meant to break us; they are meant to build us stronger. Let us not shy away from these challenges but rather face them with courage and determination.\\n\\nI urge each of you to embrace change, whether big or small. Let it inspire you to pursue your passions, to take risks, and to dream beyond the boundaries of the familiar. Surround yourselves with positivity, seek support from those who uplift you, and always keep moving forward.\\n\\nIn closing, let us celebrate the beauty of change. It is a reminder that life is a journey, filled with endless possibilities. Together, let us step boldly into the future, ready to embrace all that lies ahead.\\n\\nThank you.\\n\\n'),\n",
       " Document(metadata={'page': 8, 'source': 'attention.pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'page': 0, 'source': 'attention.pdf'}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader=PyPDFLoader(\"attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)\n",
    "final_docs=text_splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2=Chroma.from_documents(final_docs,embeddings)\n",
    "query = \"self-attention layers in the decoder\"\n",
    "retrieved_results = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 4, 'source': 'attention.pdf'}, page_content='[31, 2, 8].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       " Document(metadata={'page': 2, 'source': 'attention.pdf'}, page_content='Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This'),\n",
       " Document(metadata={'page': 4, 'source': 'attention.pdf'}, page_content='3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].'),\n",
       " Document(metadata={'page': 2, 'source': 'attention.pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [ 10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
